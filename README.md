Flat’n’Fold: A Diverse Multi-Modal Dataset for Garment Perception and Manipulation

Paper Abstract:
This repository is published along with a paper Flat’n’Fold: A Diverse Multi-Modal Dataset for Garment Perception and Manipulation.
We provided 1212 human and 887 robot demonstrations of flattening and folding 44 garments across 8 categories. Also, we establish two new benchmarks for grasping point prediction and subtask decomposition.

Dataset:
Dataset can be download at: https://gla-my.sharepoint.com/:f:/g/personal/2658047z_student_gla_ac_uk/Ekgx_o8q6ZZBtxusMwrP8zoBt2FkZL9vwq3hqe5c1CyHSQ
For each data sequences, rgb and depth image are provided. The generation of the point cloud as well as the internal and external parameter of the three cameras are provided in the pointcloud folder.

As for the code for grasping point prediction benchmark and subgoal decomposition benchmark are provided in Grasping_point and UVD folder
